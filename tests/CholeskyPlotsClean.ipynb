{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from numpywren.matrix import BigMatrix, BigSymmetricMatrix\n",
    "from numpywren import matrix_utils, uops\n",
    "from numpywren.matrix_init import shard_matrix\n",
    "import pytest\n",
    "import numpy as np\n",
    "from numpy.linalg import cholesky\n",
    "import pywren\n",
    "import unittest\n",
    "import concurrent.futures as fs\n",
    "import time\n",
    "import multiprocessing\n",
    "import os\n",
    "from importlib import reload\n",
    "from numpywren import lambdapack as lp\n",
    "from numpywren import binops\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import numpywren\n",
    "import numpywren.wait\n",
    "from numpywren import job_runner\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "import math\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dict = {}\n",
    "env_dict[\"OMP_NUM_THREADS\"] = str(1)\n",
    "env_dict[\"AWS_ACCESS_KEY_ID\"] = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "env_dict[\"AWS_SECRET_ACCESS_KEY\"] = os.environ[\"AWS_SECRET_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating X\n"
     ]
    }
   ],
   "source": [
    "N = 65536 * 4\n",
    "shard_size = 5300 \n",
    "\n",
    "#N = 1024\n",
    "#shard_size = 64 \n",
    "\n",
    "X = np.random.randn(N, 1)\n",
    "print(\"Generating X\")\n",
    "shard_sizes = (shard_size, 1)\n",
    "pwex = pywren.default_executor()\n",
    "executor = pywren.default_executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sharded = BigMatrix(\"cholesky_test_{0}_{1}\".format(N, shard_size), shape=X.shape, shard_sizes=shard_sizes, write_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpywren.matrix.BigMatrix at 0x7f14057030b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_matrix(X_sharded, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemm(BigMatrix(cholesky_test_262144_5300), BigMatrix(cholesky_test_262144_5300).T)\n",
      "Out Shape (262144, 262144)\n",
      "Total number of output blocks 1275\n",
      "Total number of output blocks that exist 1275\n",
      "Number of output blocks to generate  0\n",
      "<function _gemm_remote_0 at 0x7f13d829b400>\n",
      "CPU times: user 412 ms, sys: 0 ns, total: 412 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%time XXT_sharded = binops.gemm(pwex, X_sharded, X_sharded.T, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 924 ms, sys: 16 ms, total: 940 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "XXT_sharded.lambdav = 1\n",
    "%time instructions_full,L_sharded,trailing= lp._chol(XXT_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1275\n",
      "2500\n",
      "3676\n",
      "4804\n",
      "5885\n",
      "6920\n",
      "7910\n",
      "8856\n",
      "9759\n",
      "10620\n",
      "11440\n",
      "12220\n",
      "12961\n",
      "13664\n",
      "14330\n",
      "14960\n",
      "15555\n",
      "16116\n",
      "16644\n",
      "17140\n",
      "17605\n",
      "18040\n",
      "18446\n",
      "18824\n",
      "19175\n",
      "19500\n",
      "19800\n",
      "20076\n",
      "20329\n",
      "20560\n",
      "20770\n",
      "20960\n",
      "21131\n",
      "21284\n",
      "21420\n",
      "21540\n",
      "21645\n",
      "21736\n",
      "21814\n",
      "21880\n",
      "21935\n",
      "21980\n",
      "22016\n",
      "22044\n",
      "22065\n",
      "22080\n",
      "22090\n",
      "22096\n",
      "22099\n"
     ]
    }
   ],
   "source": [
    "for i,inst_block in enumerate(instructions_full):\n",
    "   for inst in inst_block.instrs:\n",
    "       if (inst.i_code == lp.OC.CHOL):\n",
    "           print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instructions_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TO_TRUNCATE = 5000\n",
    "NUM_CORES = 100\n",
    "NUM_MAX_CORES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = instructions_full[:INSTRUCTION_TO_TRUNCATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwex = pywren.default_executor()\n",
    "config = pwex.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serializing program before sharding\n",
      "sharding program\n",
      "Sharding program took 13.94001579284668 seconds\n",
      "[0, 1, 50, 1275, 1276, 1324, 2500, 2501, 2548, 3676, 3677, 3723, 4804, 4805, 4850, 5000]\n"
     ]
    }
   ],
   "source": [
    "program = lp.LambdaPackProgram(instructions, executor=pywren.lambda_executor, pywren_config=config, num_priorities=6)\n",
    "print(program.longest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(program.inst_blocks)\n",
    "pwex = pywren.lambda_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbdf6c210b963763877b81d10b0ead3b2e6d0a47\n"
     ]
    }
   ],
   "source": [
    "print(program.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_futures = pwex.map(lambda x: job_runner.lambdapack_run(program, pipeline_width=1, cache_size=2, timeout=100), \n",
    "#                        range(NUM_CORES), \n",
    "#                        exclude_modules=[\"site-packages\"], \n",
    "#                        extra_env={\"REDIS_IP\":os.environ[\"REDIS_IP\"]})\n",
    "all_futures = []\n",
    "all_future_set = []\n",
    "running_total = []\n",
    "waiting_total = []\n",
    "up_total = []\n",
    "measure_points = []\n",
    "\n",
    "current_task_id = 0\n",
    "client = boto3.client('sqs')\n",
    "\n",
    "def launch_more_tasks(num_launch):\n",
    "        assert num_launch > 0\n",
    "        global current_task_id\n",
    "        global client\n",
    "        global program\n",
    "        \n",
    "        for i in range(current_task_id,current_task_id+num_launch):\n",
    "            client.send_message(QueueUrl=program.worker_queue_url, MessageBody=str(i))\n",
    "        handles = []\n",
    "        while len(handles) != num_launch:\n",
    "            messages = client.receive_message(QueueUrl=program.worker_queue_url, MaxNumberOfMessages=min(10, num_launch-len(handles)))\n",
    "            if \"Messages\" not in messages:\n",
    "                continue\n",
    "            for m in messages[\"Messages\"]:\n",
    "                handles.append(m['ReceiptHandle'])\n",
    "        \n",
    "        more_futures = pwex.map(lambda x: job_runner.lambdapack_run(program, x, pipeline_width=2, cache_size=0, timeout=100, idle_timeout=10), \n",
    "                   handles, \n",
    "                   exclude_modules=[\"site-packages\"], \n",
    "                   extra_env={\"REDIS_IP\":os.environ[\"REDIS_IP\"], \"REDIS_PASS\": \"numpywren123\", \"REDIS_PORT\":\"6379\"})\n",
    "        current_task_id += num_launch\n",
    "        program.incr_pool_size(num_launch)\n",
    "        return more_futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12.721174001693726', '0', '1', '2', ' counter: ', '0', '--', '0', '2']\n",
      "['24.839184284210205', '47', '2', '24', ' counter: ', '2', '--', '0', '24']\n",
      "['37.13138723373413', '15', '34', '47', ' counter: ', '45', '--', '0', '47']\n",
      "['47.8746178150177', '0', '50', '47', ' counter: ', '52', '--', '0', '47']\n",
      "['58.76234698295593', '10', '57', '30', ' counter: ', '679', '--', '0', '30']\n",
      "['79.90936946868896', '697', '94', '534', ' counter: ', '1084', '--', '0', '534']\n",
      "['90.83994507789612', '831', '298', '534', ' counter: ', '1274', '--', '0', '534']\n",
      "['102.6289451122284', '134', '1064', '534', ' counter: ', '1275', '--', '2', '532']\n"
     ]
    }
   ],
   "source": [
    "five_point_vector = [False]*5\n",
    "five_point_pos = 0\n",
    "consecutive_count = 0\n",
    "tzero = time.time()\n",
    "scaling_factor = 0.5\n",
    "\n",
    "last_vector = []\n",
    "no_progress = 0\n",
    "while(program.program_status() == lp.PS.RUNNING):\n",
    "    time.sleep(1)\n",
    "    waiting = 0\n",
    "    running = 0\n",
    "    client = boto3.client('sqs')\n",
    "    for p,queue_url in enumerate(program.queue_urls):\n",
    "        #print(\"Priority {0}\".format(p))\n",
    "        attrs = client.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible'])['Attributes']\n",
    "        #print(attrs)\n",
    "        waiting += int(attrs[\"ApproximateNumberOfMessages\"])\n",
    "        running += int(attrs[\"ApproximateNumberOfMessagesNotVisible\"])\n",
    "        # Implement scale up policy here\n",
    "    attrs = client.get_queue_attributes(QueueUrl=program.worker_queue_url, AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible'])['Attributes']\n",
    "    w_waiting = int(attrs[\"ApproximateNumberOfMessages\"])\n",
    "    w_running = int(attrs[\"ApproximateNumberOfMessagesNotVisible\"])\n",
    "\n",
    "    five_point_pos = (five_point_pos + 1) % len(five_point_vector)\n",
    "    five_point_vector[five_point_pos] = waiting > 0\n",
    "    up = int(program.get_pool_size())\n",
    "    w_up = w_running\n",
    "    #if (waiting > 0 and up < waiting + running and up < NUM_MAX_CORES):\n",
    "    #    all_futures += launch_more_tasks(min(waiting, waiting+running-up, NUM_MAX_CORES-up))\n",
    "    #if w_up != 1:\n",
    "    #    all_future_set.append(launch_more_tasks(1))\n",
    "    #if up != 1:\n",
    "    #    all_future_set.append(launch_more_tasks(1))\n",
    "    #if (up < math.ceil(1.0*(waiting)*scaling_factor) and up < NUM_MAX_CORES):\n",
    "    #    all_future_set.append(launch_more_tasks(min(int(math.ceil(1.0*(waiting)*scaling_factor-up)), NUM_MAX_CORES-up)))\n",
    "    if (w_up < math.ceil(1.0*(waiting)*scaling_factor) and up < NUM_MAX_CORES):\n",
    "        all_future_set.append(launch_more_tasks(min(int(math.ceil(1.0*(waiting)*scaling_factor-w_up)), NUM_MAX_CORES-w_up)))\n",
    "    \n",
    "#     if five_point_pos == (len(five_point_vector) - 1):\n",
    "#         if all(five_point_vector):\n",
    "#             all_futures += launch_more_tasks(2**consecutive_count)\n",
    "#             consecutive_count += 1\n",
    "#         else:\n",
    "#             consecutive_count = 0\n",
    "    waiting_total.append(waiting)\n",
    "    running_total.append(running)\n",
    "    up_total.append(up)\n",
    "    measure_points.append(time.time() - tzero)\n",
    "    if last_vector == [waiting, running, up]:\n",
    "        no_progress += 1\n",
    "        if no_progress == 30:\n",
    "            program.decr_pool_size(up)\n",
    "            print(\"resetting counter.\")\n",
    "    else:\n",
    "        last_vector = [waiting, running, up]\n",
    "        no_progress = 0\n",
    "    if len(waiting_total) % 10 == 0:\n",
    "        output_vec = [time.time() - tzero, waiting, running, up, \" counter: \", program.get_max_pc(), \"--\", w_waiting, w_running]\n",
    "        output_str = [str(item) for item in output_vec]\n",
    "        print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting = 0\n",
    "# running = 0\n",
    "# for queue_url in program.queue_urls[::-1]:\n",
    "#     attrs = client.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible'])['Attributes']\n",
    "#     #print(attrs)\n",
    "#     waiting += int(attrs[\"ApproximateNumberOfMessages\"])\n",
    "#     running += int(attrs[\"ApproximateNumberOfMessagesNotVisible\"])\n",
    "#     messages = client.receive_message(QueueUrl=queue_url, MaxNumberOfMessages=1)\n",
    "#     if (\"Messages\" not in messages):\n",
    "#         break\n",
    "#         #continue\n",
    "#     else:\n",
    "#         break\n",
    "# if (\"Messages\" not in messages):\n",
    "#     print(\"no msg: \" + str(waiting) + \" \" + str(running))\n",
    "# else:\n",
    "#     print(\"msg: \" + str(waiting) + \" \" + str(running))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = trapz(up_total, measure_points)\n",
    "duration = max(measure_points)\n",
    "print(\"duration: \" + str(duration) + \" cost: \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_futures = [f for fs in all_future_set for f in fs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(measure_points) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[f.result()['up_time'][1] - f.result()['up_time'][0] for f in all_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = all_futures[0]\n",
    "#f.run_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(program.program_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(program.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = fs.ThreadPoolExecutor(64)\n",
    "futures = []\n",
    "for i in range(0,len(program.inst_blocks),1):\n",
    "    \n",
    "    futures.append(executor.submit(program.get_profiling_info, i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(futures))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time res = fs.wait(futures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_blocks = []\n",
    "for f in futures:\n",
    "    try:\n",
    "        profiled_blocks.append(f.result())\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches as mpatches\n",
    "time_offset = np.min([i.start_time for b in profiled_blocks for i in b.instrs if i.start_time != None and i.end_time != None])\n",
    "instructions = [i for b in profiled_blocks for i in b.instrs if i.start_time != None and i.end_time != None]\n",
    "fig = pylab.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "total_jobs = len(instructions)\n",
    "y = np.arange(len(instructions))\n",
    "point_size = 50\n",
    "\n",
    "start_times = np.array([i.start_time for i in instructions]) - time_offset\n",
    "end_times = np.array([i.end_time for i in instructions]) - time_offset\n",
    "palette = sns.color_palette(\"deep\", 6)\n",
    "patches = []\n",
    "\n",
    "READ_INSTRUCTIONS = [lp.OC.S3_LOAD]\n",
    "WRITE_INSTRUCTIONS = [lp.OC.S3_WRITE]\n",
    "COMPUTE_INSTRUCTIONS = [lp.OC.SYRK, lp.OC.TRSM, lp.OC.INVRS, lp.OC.CHOL]\n",
    "\n",
    "read_starts = []\n",
    "read_ends = []\n",
    "write_starts = []\n",
    "write_ends = []\n",
    "compute_starts = []\n",
    "compute_ends = []\n",
    "read_ys = []\n",
    "write_ys = []\n",
    "compute_ys = []\n",
    "write_times = []\n",
    "compute_times = []\n",
    "read_times = []\n",
    "for j,block in enumerate(profiled_blocks):\n",
    "    syrk = False\n",
    "    for i in block.instrs:\n",
    "        if i.start_time == None or i.end_time == None: \n",
    "            continue\n",
    "        if (i.i_code in READ_INSTRUCTIONS):\n",
    "            read_starts.append(i.start_time - time_offset)\n",
    "            read_ends.append(i.end_time - time_offset)\n",
    "            read_times.append(i.end_time - i.start_time)\n",
    "            read_ys.append(j) \n",
    "            \n",
    "        elif (i.i_code in COMPUTE_INSTRUCTIONS):\n",
    "            if (i.i_code == lp.OC.SYRK):\n",
    "                syrk = True\n",
    "                compute_times.append(i.end_time - i.start_time)\n",
    "            compute_starts.append(i.start_time - time_offset)\n",
    "            compute_ends.append(i.end_time - time_offset)\n",
    "            compute_ys.append(j)\n",
    "        elif (i.i_code in WRITE_INSTRUCTIONS):\n",
    "            \n",
    "            if (syrk):\n",
    "                write_starts.append(i.start_time - time_offset)\n",
    "                write_ends.append(i.end_time - time_offset)\n",
    "                write_times.append(write_ends[-1] - write_starts[-1])\n",
    "                write_ys.append(j)\n",
    "       \n",
    "\n",
    "for i,instr in enumerate(instructions):\n",
    "    if (instr.i_code == lp.OC.CHOL):\n",
    "        ax.axvline(instructions[i+1].start_time - time_offset, color=\"black\", linestyle='--')\n",
    "        ax.axvline(instructions[i+1].end_time - time_offset, color=\"black\")\n",
    "        \n",
    "        \n",
    "        \n",
    "ax.scatter(read_starts, read_ys,  c=palette[0], edgecolor='none', s=point_size, alpha=0.8)\n",
    "patches.append(mpatches.Patch(color=palette[0], label=\"read_argument\"))\n",
    "ax.scatter(read_ends, read_ys,  c=palette[1], edgecolor='none', s=point_size, alpha=0.8)\n",
    "patches.append(mpatches.Patch(color=palette[1], label=\"read_end\"))\n",
    "\n",
    "ax.scatter(compute_starts, compute_ys,  c=palette[2], edgecolor='none', s=point_size, alpha=0.8)\n",
    "patches.append(mpatches.Patch(color=palette[2], label=\"compute_start\"))\n",
    "ax.scatter(compute_ends, compute_ys,  c=palette[3], edgecolor='none', s=point_size, alpha=0.8)\n",
    "patches.append(mpatches.Patch(color=palette[3], label=\"compute_end\"))\n",
    "\n",
    "ax.scatter(write_starts, write_ys,  c=palette[4], edgecolor='none', s=point_size, alpha=0.2)\n",
    "patches.append(mpatches.Patch(color=palette[4], label=\"write_start\"))\n",
    "ax.scatter(write_ends, write_ys,  c=palette[5], edgecolor='none', s=point_size, alpha=0.2)\n",
    "patches.append(mpatches.Patch(color=palette[5], label=\"write_result\"))\n",
    "ax.set_xlabel('wallclock time (sec)')\n",
    "ax.set_ylabel('job')\n",
    "\n",
    "legend = pylab.legend(handles=patches, \n",
    "                      loc='upper left', frameon=True)\n",
    "#pylab.title(\"Runtime for {} jobs of {:3.0f}M double ops (dgemm) each\".format(total_jobs, JOB_GFLOPS))\n",
    "legend.get_frame().set_facecolor('#FFFFFF')\n",
    "#ax.set_ylim(2500,2510)\n",
    "#ax.set_xticks(np.arange(20))\n",
    "#ax.set_xlim(70,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(read_times, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(compute_times, bins=100)\n",
    "plt.xlabel(\"Low Rank Update Time\")\n",
    "plt.ylabel(\"Number of jobs completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read,write,total_flops,bins, instructions, runtimes = lp.perf_profile(profiled_blocks, num_bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimes = defaultdict(int)\n",
    "opcounts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inst, t in zip(instructions, runtimes):\n",
    "    opcounts[inst.i_code] += 1\n",
    "    optimes[inst.i_code] += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in optimes.keys():\n",
    "    print(\"{0}: {1}s\".format(k, optimes[k]/opcounts[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profiled_blocks[32].end_time - profiled_blocks[0].start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in optimes.keys():\n",
    "    print(\"{0}: {1}s\".format(k, optimes[k]/opcounts[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(read, time=(bins - min(bins)))\n",
    "plt.xlabel(\"time since start\")\n",
    "plt.ylabel(\"aggregate read GB/s\")\n",
    "#plt.xlim(10,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(write, time=(bins - min(bins)))\n",
    "plt.xlabel(\"time since start\")\n",
    "plt.ylabel(\"aggregate write GB/s\")\n",
    "#plt.xlim(10,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(3*sum(total_flops), 1.0/3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sns.palettes.color_palette()[1]\n",
    "sns.tsplot(total_flops, time=(bins - min(bins)), condition=\"Observed Performance\")\n",
    "plt.xlabel(\"Time since start\")\n",
    "plt.ylabel(\"aggregate Gflops/s\")\n",
    "plt.hlines(20*NUM_CORES,0,1000, label=\"Theoretical Performance\", color=c)\n",
    "plt.legend(bbox_to_anchor=(1.5, 0.8))\n",
    "#plt.savefig(\"flops_100k.pdf\")\n",
    "#plt.xlim(9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((16384**3)/3)/(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this does not work in current PyWren as it has multiple callsets\n",
    "# pywren.wait(all_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures as fss\n",
    "import redis\n",
    "#import pywren.storage.StorageOutputNotFoundError\n",
    "\n",
    "all_results = []\n",
    "bad_counts = 0\n",
    "bad_counts_1 = 0\n",
    "bad_counts_2 = 0\n",
    "bad_counts_3 = 0\n",
    "bad_future = None\n",
    "count = 0\n",
    "for fs in all_future_set[:20]:\n",
    "    #print(len(all_results))\n",
    "    try:\n",
    "        pywren.wait(fs)\n",
    "    except Exception as e:\n",
    "        print(\"wait error\")\n",
    "    for f in fs:\n",
    "        count += 1\n",
    "        #print(count)\n",
    "        try:\n",
    "           result = f.result()\n",
    "           all_results.append(result)\n",
    "        except fss._base.TimeoutError as e:\n",
    "            print(e)\n",
    "            bad_counts += 1\n",
    "            continue\n",
    "        except redis.connection.ResponseError as e:\n",
    "            #print(e)\n",
    "            bad_counts_1 += 1\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            if str(e) == \"process ran out of time\":\n",
    "                bad_counts_2 += 1\n",
    "            else:\n",
    "                bad_counts_3 += 1\n",
    "                print(e)\n",
    "            continue\n",
    "print(\"bad counts: \" + str(bad_counts))\n",
    "print(\"bad counts_1: \" + str(bad_counts_1))\n",
    "print(\"bad counts_2: \" + str(bad_counts_2))\n",
    "print(\"bad counts_3: \" + str(bad_counts_3))\n",
    "# all_results = [f.result() for f in all_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_future_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_times = [exec_time for res in all_results for exec_time in res['exec_time']]\n",
    "tasks_per_worker = [len(res['exec_time']) for res in all_results]\n",
    "up_times = [res['up_time'] for res in all_results]\n",
    "#print((up_times))\n",
    "\n",
    "print(len(exec_times))\n",
    "#gap_times = [res['up_time'][1] - max([max(t) for t in res['exec_time']] + [res['up_time'][1]]) for res in all_results]\n",
    "up_duration = [up[1] - up[0] for up in up_times]\n",
    "exec_duration = [up[1] - up[0] for up in exec_times]\n",
    "plt.hist(up_duration, bins=100)\n",
    "plt.xlabel(\"task execution time (seconds)\")\n",
    "plt.ylabel(\"counts (seconds)\")\n",
    "plt.show()\n",
    "# print(up_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runtime_bins = np.linspace(0, int(max(measure_points) + 1), int(max(measure_points) + 1) * 2)\n",
    "\n",
    "def compute_times_rates(d, tzero):\n",
    "    \n",
    "    x = np.array(d)\n",
    "    # tzero = np.min(x[:, 0])\n",
    "    start_time = x[:, 0] - tzero\n",
    "    end_time = x[:, 1]  - tzero\n",
    "\n",
    "    N = len(start_time)\n",
    "\n",
    "    runtime_jobs_hist = np.zeros((N, len(runtime_bins)))\n",
    "\n",
    "    for i in range(N):\n",
    "        s = start_time[i]\n",
    "        e = end_time[i]\n",
    "        a, b = np.searchsorted(runtime_bins, [s, e])\n",
    "        if b-a > 0:\n",
    "            runtime_jobs_hist[i, a:b] = 1\n",
    "\n",
    "    return {'start_time' : start_time, \n",
    "            'end_time' : end_time, \n",
    "            'runtime_jobs_hist' : runtime_jobs_hist}\n",
    "\n",
    "exec_timeline = compute_times_rates(exec_times, tzero)\n",
    "up_timeline = compute_times_rates(up_times, tzero)\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "    \n",
    "fig = pylab.figure(figsize=(8, 4))\n",
    "\n",
    "current_palette = sns.color_palette()\n",
    "up_color = current_palette[0]\n",
    "exec_color = current_palette[1]\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for plot_i, (datum, l, c) in enumerate([(exec_timeline, 'busy workers', up_color), \n",
    "                                    (up_timeline, 'ready workers', exec_color)]):\n",
    "\n",
    "    ax.plot(runtime_bins, datum['runtime_jobs_hist'].sum(axis=0), \n",
    "            c=c, label=l, \n",
    "           zorder=-1)\n",
    "\n",
    "\n",
    "    ax.set_xlim(0, np.max(datum['end_time']))\n",
    "    ax.set_xlabel(\"time (sec)\")\n",
    "\n",
    "#ax.plot(measure_points, running_total, 'r', label=\"running tasks\")\n",
    "#ax.plot(measure_points, waiting_total, 'g', label=\"tasks in queue\")\n",
    "ax.plot(measure_points, up_total, 'black', label=\"up workers\")\n",
    "\n",
    "ax.set_ylim(0, max(up_total)*1.05)\n",
    "ax.set_ylabel(\"num workers\")\n",
    "ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pylab.figure(figsize=(8, 4))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(measure_points, running_total, 'r', label=\"running tasks\")\n",
    "ax.plot(measure_points, waiting_total, 'g', label=\"tasks in queue\")\n",
    "ax.plot(measure_points, up_total, 'black', label=\"up counter\")\n",
    "ax.set_ylabel(\"num workers\")\n",
    "ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busy_area = trapz(exec_timeline['runtime_jobs_hist'].sum(axis=0), runtime_bins)\n",
    "ready_area = trapz(up_timeline['runtime_jobs_hist'].sum(axis=0), runtime_bins)\n",
    "up_area = trapz(up_total, measure_points)\n",
    "print({\"effec_util\":busy_area/ready_area,\n",
    "       \"startup_overhead\": 1-ready_area/up_area,\n",
    "       \"overall_util\":busy_area/up_area})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(runtime_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = {\"runtime_bins\": runtime_bins,\n",
    "          \"exec_timeline\": exec_timeline,\n",
    "          \"up_timeline\": up_timeline,\n",
    "          \"measure_points\": measure_points,\n",
    "          \"running_total\": running_total,\n",
    "          \"waiting_total\": waiting_total,\n",
    "          \"up_total\": up_total}\n",
    "pickle.dump(results, open('cholesky_256k_moderate_10s.pickle', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load(open('cholesky_256k_moderate_10s.pickle', 'rb'))\n",
    "up_total = results[\"up_total\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pylab.figure(figsize=(8, 4))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "c = sns.palettes.color_palette()\n",
    "sns.tsplot(total_flops, time=(bins - min(bins)), condition=\"Observed Performance\", color=c[1])\n",
    "sns.tsplot(np.multiply(up_total, 20), time=measure_points, condition=\"Theoretical Peak Performance\", color=c[2])\n",
    "sns.tsplot(np.multiply(exec_timeline['runtime_jobs_hist'].sum(axis=0), 20), time=runtime_bins, condition=\"Theoretical Running Performance\", color=c[3])\n",
    "#sns.tsplot(np.multiply(up_timeline['runtime_jobs_hist'].sum(axis=0), 20), time=runtime_bins, condition=\"Theoretical Available Performance\", color=c[4])\n",
    "\n",
    "plt.xlabel(\"Time since start\")\n",
    "plt.ylabel(\"aggregate Gflops/s\")\n",
    "# plt.hlines(20*NUM_CORES,0,1000, label=\"Theoretical Performance\", color=c)\n",
    "plt.legend(bbox_to_anchor=(1.5, 0.8))\n",
    "plt.savefig(\"flops_autoscaling.pdf\")\n",
    "#plt.xlim(9,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapz(total_flops, bins) / busy_area / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_type_count = {}\n",
    "for f in all_futures:\n",
    "    if f.done():\n",
    "        model = f.run_status['server_info']['/proc/cpuinfo'].split(\"\\n\")[4]\n",
    "        if model not in cpu_type_count:\n",
    "            print (f.run_status['server_info']['/proc/cpuinfo'].split(\"\\n\"))\n",
    "            cpu_type_count[model] = 0\n",
    "        cpu_type_count[model] += 1\n",
    "print(cpu_type_count)\n",
    "\n",
    "32\n",
    "18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
